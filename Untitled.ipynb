{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a15c74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading EEG Dataset...\n",
      "\n",
      "Training Fold 1...\n",
      "Epoch 1/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - loss: 0.1852 - val_loss: 0.0014\n",
      "Epoch 2/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 3/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 4/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 5/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 6/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 7/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 8/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 9/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 10/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"functional_5\" is incompatible with the layer: expected shape=(None, 993), found shape=(None, 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 123\u001b[0m\n\u001b[0;32m    120\u001b[0m         gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 123\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 111\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    109\u001b[0m classifier_model \u001b[38;5;241m=\u001b[39m Model(encoder\u001b[38;5;241m.\u001b[39minput, classifier)\n\u001b[0;32m    110\u001b[0m classifier_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 111\u001b[0m \u001b[43mclassifier_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# Evaluate Model\u001b[39;00m\n\u001b[0;32m    114\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(classifier_model\u001b[38;5;241m.\u001b[39mpredict(X_val_encoded), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py:245\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[1;32m--> 245\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    246\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    250\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"functional_5\" is incompatible with the layer: expected shape=(None, 993), found shape=(None, 32)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import gc\n",
    "from scipy.fft import dct, idct\n",
    "from scipy.stats import skew, kurtosis, entropy\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Fourier Decomposition Method (FDM)\n",
    "def fdm(X, fs, fc):\n",
    "    N = X.shape[0]\n",
    "    fc = np.sort(fc)\n",
    "\n",
    "    if fc[0] != 0:\n",
    "        fc = np.hstack((0, fc))\n",
    "    if fc[-1] != fs / 2:\n",
    "        fc = np.hstack((fc, fs / 2))\n",
    "\n",
    "    dct_type = 2\n",
    "    K = np.round(2 * N * fc / fs).astype(int)\n",
    "    Hk = np.zeros((N, 1, len(K) - 1))\n",
    "\n",
    "    for i in range(len(K) - 1):\n",
    "        Hk[K[i]:K[i + 1], :, i] = 1\n",
    "\n",
    "    Xk = dct(X, type=dct_type, n=N, axis=0, norm='ortho')\n",
    "    Yk = np.einsum('ij,ijk->ijk', Xk, Hk)\n",
    "    Y = idct(Yk, type=dct_type, n=N, axis=0, norm='ortho')\n",
    "\n",
    "    return np.squeeze(Y) if X.ndim == 1 else Y\n",
    "\n",
    "# Feature Extraction\n",
    "def extract_features(X):\n",
    "    mean_amplitude = np.mean(X, axis=1, keepdims=True)  # Shape (n_samples, 1)\n",
    "    variance = np.var(X, axis=1, keepdims=True)         # Shape (n_samples, 1)\n",
    "    skewness = skew(X, axis=1, keepdims=True)           # Shape (n_samples, 1)\n",
    "    kurt = kurtosis(X, axis=1, keepdims=True)           # Shape (n_samples, 1)\n",
    "    \n",
    "    # Normalize for entropy calculation\n",
    "    X_normalized = np.abs(X) / np.sum(np.abs(X), axis=1, keepdims=True)\n",
    "    signal_entropy = np.apply_along_axis(entropy, axis=1, arr=X_normalized).reshape(-1, 1)  # Reshape to (n_samples, 1)\n",
    "    \n",
    "    return np.hstack([mean_amplitude, variance, skewness, kurt, signal_entropy])\n",
    "\n",
    "# Preprocess Data\n",
    "def preprocess_data(df, label_column='Label', normalize=True):\n",
    "    labels = df[label_column].values\n",
    "    features = df.drop(columns=[label_column]).values\n",
    "    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)  # Replace NaN/Inf\n",
    "    additional_features = extract_features(features)\n",
    "    features = np.hstack([features, additional_features])  # Append additional features\n",
    "    if normalize:\n",
    "        features = features / np.max(np.abs(features), axis=1, keepdims=True)  # Normalize each row\n",
    "    return features, labels\n",
    "\n",
    "# Autoencoder Model\n",
    "def build_autoencoder(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    encoded = Dense(128, activation='relu')(inputs)\n",
    "    encoded = Dense(64, activation='relu')(encoded)\n",
    "    encoded = Dense(32, activation='relu')(encoded)\n",
    "    decoded = Dense(64, activation='relu')(encoded)\n",
    "    decoded = Dense(128, activation='relu')(decoded)\n",
    "    decoded = Dense(input_shape[0], activation='sigmoid')(decoded)\n",
    "    \n",
    "    autoencoder = Model(inputs, decoded)\n",
    "    encoder = Model(inputs, encoded)\n",
    "    autoencoder.compile(optimizer=Adam(), loss='mse')\n",
    "    return autoencoder, encoder\n",
    "\n",
    "# Main Function\n",
    "def main():\n",
    "    # File paths\n",
    "    eeg_file = 'mental-state.csv'  # Replace with your dataset path\n",
    "    \n",
    "    # Load and preprocess EEG Dataset\n",
    "    print(\"\\nLoading EEG Dataset...\")\n",
    "    eeg_df = pd.read_csv(eeg_file)\n",
    "    X, y = preprocess_data(eeg_df)\n",
    "\n",
    "    # Compute class weights to handle class imbalance\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "    class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "    # Initialize K-Fold Cross Validation\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    fold_idx = 1\n",
    "\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        print(f\"\\nTraining Fold {fold_idx}...\")\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        # Train Autoencoder\n",
    "        autoencoder, encoder = build_autoencoder((X_train.shape[1],))\n",
    "        autoencoder.fit(X_train, X_train, epochs=10, batch_size=32, validation_data=(X_val, X_val), verbose=1)\n",
    "\n",
    "        # Extract Features using Encoder\n",
    "        X_train_encoded = encoder.predict(X_train)\n",
    "        X_val_encoded = encoder.predict(X_val)\n",
    "\n",
    "        # Train a simple classifier\n",
    "        classifier = Dense(len(np.unique(y)), activation='softmax')(encoder.output)\n",
    "        classifier_model = Model(encoder.input, classifier)\n",
    "        classifier_model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        classifier_model.fit(X_train_encoded, y_train, epochs=10, batch_size=32, validation_data=(X_val_encoded, y_val), verbose=1)\n",
    "\n",
    "        # Evaluate Model\n",
    "        y_pred = np.argmax(classifier_model.predict(X_val_encoded), axis=1)\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        print(f\"Fold {fold_idx} Accuracy: {accuracy:.4f}\")\n",
    "        print(classification_report(y_val, y_pred))\n",
    "        \n",
    "        fold_idx += 1\n",
    "        gc.collect()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6d33b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading EEG Dataset...\n",
      "Epoch 1/10\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.1673 - val_loss: 0.0014\n",
      "Epoch 2/10\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 3/10\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 4/10\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 5/10\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 6/10\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 7/10\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 8/10\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 9/10\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 10/10\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Epoch 1/10\n",
      "\u001b[1m53/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3875 - loss: 1.4505"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 106\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28mprint\u001b[39m(classification_report(y_val, y_pred))\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 106\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 98\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     96\u001b[0m classifier_model \u001b[38;5;241m=\u001b[39m Model(input_encoded, classifier_output)\n\u001b[0;32m     97\u001b[0m classifier_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 98\u001b[0m \u001b[43mclassifier_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(classifier_model\u001b[38;5;241m.\u001b[39mpredict(X_val_encoded), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    101\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_val, y_pred)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\data_adapter_utils.py:103\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_data_cardinality\u001b[39m(data):\n\u001b[1;32m--> 103\u001b[0m     num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mint\u001b[39m(\u001b[43mi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mflatten(data))\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(num_samples) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    105\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    106\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData cardinality is ambiguous. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    107\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    108\u001b[0m         )\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import gc\n",
    "from scipy.fft import dct, idct\n",
    "from scipy.stats import skew, kurtosis, entropy\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Fourier Decomposition Method (FDM)\n",
    "def fdm(X, fs, fc):\n",
    "    N = X.shape[0]\n",
    "    fc = np.sort(fc)\n",
    "\n",
    "    if fc[0] != 0:\n",
    "        fc = np.hstack((0, fc))\n",
    "    if fc[-1] != fs / 2:\n",
    "        fc = np.hstack((fc, fs / 2))\n",
    "\n",
    "    dct_type = 2\n",
    "    K = np.round(2 * N * fc / fs).astype(int)\n",
    "    Hk = np.zeros((N, 1, len(K) - 1))\n",
    "\n",
    "    for i in range(len(K) - 1):\n",
    "        Hk[K[i]:K[i + 1], :, i] = 1\n",
    "\n",
    "    Xk = dct(X, type=dct_type, n=N, axis=0, norm='ortho')\n",
    "    Yk = np.einsum('ij,ijk->ijk', Xk, Hk)\n",
    "    Y = idct(Yk, type=dct_type, n=N, axis=0, norm='ortho')\n",
    "\n",
    "    return np.squeeze(Y) if X.ndim == 1 else Y\n",
    "\n",
    "# Feature Extraction\n",
    "def extract_features(X):\n",
    "    mean_amplitude = np.mean(X, axis=1, keepdims=True)\n",
    "    variance = np.var(X, axis=1, keepdims=True)\n",
    "    skewness = skew(X, axis=1, keepdims=True)\n",
    "    kurt = kurtosis(X, axis=1, keepdims=True)\n",
    "    \n",
    "    X_normalized = np.abs(X) / np.sum(np.abs(X), axis=1, keepdims=True)\n",
    "    signal_entropy = np.apply_along_axis(entropy, axis=1, arr=X_normalized).reshape(-1, 1)\n",
    "    \n",
    "    return np.hstack([mean_amplitude, variance, skewness, kurt, signal_entropy])\n",
    "\n",
    "# Preprocess Data\n",
    "def preprocess_data(df, label_column='Label', normalize=True):\n",
    "    labels = df[label_column].values\n",
    "    features = df.drop(columns=[label_column]).values\n",
    "    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    additional_features = extract_features(features)\n",
    "    features = np.hstack([features, additional_features])\n",
    "    if normalize:\n",
    "        features = features / np.max(np.abs(features), axis=1, keepdims=True)\n",
    "    return features, labels\n",
    "\n",
    "# Autoencoder Model\n",
    "def build_autoencoder(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    encoded = Dense(128, activation='relu')(inputs)\n",
    "    encoded = Dense(64, activation='relu')(encoded)\n",
    "    encoded = Dense(32, activation='relu')(encoded)\n",
    "    decoded = Dense(64, activation='relu')(encoded)\n",
    "    decoded = Dense(128, activation='relu')(decoded)\n",
    "    decoded = Dense(input_shape[0], activation='sigmoid')(decoded)\n",
    "    \n",
    "    autoencoder = Model(inputs, decoded)\n",
    "    encoder = Model(inputs, encoded)\n",
    "    autoencoder.compile(optimizer=Adam(), loss='mse')\n",
    "    return autoencoder, encoder\n",
    "\n",
    "# Main Function\n",
    "def main():\n",
    "    eeg_file = 'mental-state.csv'\n",
    "    \n",
    "    print(\"\\nLoading EEG Dataset...\")\n",
    "    eeg_df = pd.read_csv(eeg_file)\n",
    "    X, y = preprocess_data(eeg_df)\n",
    "\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "    class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "    X_train, X_val = X[:int(0.8 * len(X))], X[int(0.8 * len(X)):]\n",
    "    y_train, y_val = y[:int(0.8 * len(y))], y[int(0.8 * len(y))]\n",
    "\n",
    "    autoencoder, encoder = build_autoencoder((X_train.shape[1],))\n",
    "    autoencoder.fit(X_train, X_train, epochs=10, batch_size=32, validation_data=(X_val, X_val), verbose=1)\n",
    "\n",
    "    X_train_encoded = encoder.predict(X_train)\n",
    "    X_val_encoded = encoder.predict(X_val)\n",
    "\n",
    "    input_encoded = Input(shape=(32,))\n",
    "    classifier_output = Dense(len(np.unique(y)), activation='softmax')(input_encoded)\n",
    "    classifier_model = Model(input_encoded, classifier_output)\n",
    "    classifier_model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    classifier_model.fit(X_train_encoded, y_train, epochs=10, batch_size=32, validation_data=(X_val_encoded, y_val), verbose=1)\n",
    "\n",
    "    y_pred = np.argmax(classifier_model.predict(X_val_encoded), axis=1)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    print(f\"\\nFinal Model Accuracy: {accuracy:.4f}\")\n",
    "    print(classification_report(y_val, y_pred))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16304dbd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_encoded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Print shape to debug\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoded Train Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mX_train_encoded\u001b[49m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoded Val Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_val_encoded\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_encoded' is not defined"
     ]
    }
   ],
   "source": [
    "# Print shape to debug\n",
    "print(f\"Encoded Train Shape: {X_train_encoded.shape}\")\n",
    "print(f\"Encoded Val Shape: {X_val_encoded.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f473599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading EEG Dataset...\n",
      "Epoch 1/10\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 0.1678 - val_loss: 0.0015\n",
      "Epoch 2/10\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 3/10\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 4/10\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 5/10\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 6/10\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 7/10\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 8/10\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 9/10\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 10/10\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Epoch 1/10\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.2764 - loss: 1.3068 - val_accuracy: 0.3548 - val_loss: 0.9419\n",
      "Epoch 2/10\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4266 - loss: 0.9461 - val_accuracy: 0.6532 - val_loss: 0.9027\n",
      "Epoch 3/10\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6464 - loss: 0.8993 - val_accuracy: 0.6532 - val_loss: 0.8653\n",
      "Epoch 4/10\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6495 - loss: 0.8592 - val_accuracy: 0.6431 - val_loss: 0.8349\n",
      "Epoch 5/10\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6267 - loss: 0.8434 - val_accuracy: 0.6411 - val_loss: 0.8131\n",
      "Epoch 6/10\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6512 - loss: 0.8173 - val_accuracy: 0.6573 - val_loss: 0.7880\n",
      "Epoch 7/10\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6598 - loss: 0.7888 - val_accuracy: 0.6774 - val_loss: 0.7707\n",
      "Epoch 8/10\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6578 - loss: 0.7730 - val_accuracy: 0.6532 - val_loss: 0.7588\n",
      "Epoch 9/10\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6685 - loss: 0.7615 - val_accuracy: 0.6835 - val_loss: 0.7444\n",
      "Epoch 10/10\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6648 - loss: 0.7428 - val_accuracy: 0.6855 - val_loss: 0.7329\n",
      "Epoch 1: Training Accuracy = 0.2980\n",
      "Epoch 2: Training Accuracy = 0.5275\n",
      "Epoch 3: Training Accuracy = 0.6379\n",
      "Epoch 4: Training Accuracy = 0.6440\n",
      "Epoch 5: Training Accuracy = 0.6425\n",
      "Epoch 6: Training Accuracy = 0.6500\n",
      "Epoch 7: Training Accuracy = 0.6490\n",
      "Epoch 8: Training Accuracy = 0.6556\n",
      "Epoch 9: Training Accuracy = 0.6646\n",
      "Epoch 10: Training Accuracy = 0.6606\n",
      "Epoch 1: Validation Accuracy = 0.3548\n",
      "Epoch 2: Validation Accuracy = 0.6532\n",
      "Epoch 3: Validation Accuracy = 0.6532\n",
      "Epoch 4: Validation Accuracy = 0.6431\n",
      "Epoch 5: Validation Accuracy = 0.6411\n",
      "Epoch 6: Validation Accuracy = 0.6573\n",
      "Epoch 7: Validation Accuracy = 0.6774\n",
      "Epoch 8: Validation Accuracy = 0.6532\n",
      "Epoch 9: Validation Accuracy = 0.6835\n",
      "Epoch 10: Validation Accuracy = 0.6855\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\n",
      "Final Model Accuracy: 0.6855\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.70      0.76      0.73       176\n",
      "         1.0       0.63      0.30      0.41       162\n",
      "         2.0       0.69      0.99      0.82       158\n",
      "\n",
      "    accuracy                           0.69       496\n",
      "   macro avg       0.67      0.69      0.65       496\n",
      "weighted avg       0.67      0.69      0.65       496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import gc\n",
    "from scipy.fft import dct, idct\n",
    "from scipy.stats import skew, kurtosis, entropy\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Fourier Decomposition Method (FDM)\n",
    "def fdm(X, fs, fc):\n",
    "    N = X.shape[0]\n",
    "    fc = np.sort(fc)\n",
    "\n",
    "    if fc[0] != 0:\n",
    "        fc = np.hstack((0, fc))\n",
    "    if fc[-1] != fs / 2:\n",
    "        fc = np.hstack((fc, fs / 2))\n",
    "\n",
    "    dct_type = 2\n",
    "    K = np.round(2 * N * fc / fs).astype(int)\n",
    "    Hk = np.zeros((N, 1, len(K) - 1))\n",
    "\n",
    "    for i in range(len(K) - 1):\n",
    "        Hk[K[i]:K[i + 1], :, i] = 1\n",
    "\n",
    "    Xk = dct(X, type=dct_type, n=N, axis=0, norm='ortho')\n",
    "    Yk = np.einsum('ij,ijk->ijk', Xk, Hk)\n",
    "    Y = idct(Yk, type=dct_type, n=N, axis=0, norm='ortho')\n",
    "\n",
    "    return np.squeeze(Y) if X.ndim == 1 else Y\n",
    "\n",
    "# Feature Extraction\n",
    "def extract_features(X):\n",
    "    mean_amplitude = np.mean(X, axis=1, keepdims=True)\n",
    "    variance = np.var(X, axis=1, keepdims=True)\n",
    "    skewness = skew(X, axis=1, keepdims=True)\n",
    "    kurt = kurtosis(X, axis=1, keepdims=True)\n",
    "    \n",
    "    X_normalized = np.abs(X) / np.sum(np.abs(X), axis=1, keepdims=True)\n",
    "    signal_entropy = np.apply_along_axis(entropy, axis=1, arr=X_normalized).reshape(-1, 1)\n",
    "    \n",
    "    return np.hstack([mean_amplitude, variance, skewness, kurt, signal_entropy])\n",
    "\n",
    "# Preprocess Data\n",
    "def preprocess_data(df, label_column='Label', normalize=True):\n",
    "    labels = df[label_column].values\n",
    "    features = df.drop(columns=[label_column]).values\n",
    "    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    additional_features = extract_features(features)\n",
    "    features = np.hstack([features, additional_features])\n",
    "    if normalize:\n",
    "        features = features / np.max(np.abs(features), axis=1, keepdims=True)\n",
    "    return features, labels\n",
    "\n",
    "# Autoencoder Model\n",
    "def build_autoencoder(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    encoded = Dense(128, activation='relu')(inputs)\n",
    "    encoded = Dense(64, activation='relu')(encoded)\n",
    "    encoded = Dense(32, activation='relu')(encoded)\n",
    "    decoded = Dense(64, activation='relu')(encoded)\n",
    "    decoded = Dense(128, activation='relu')(decoded)\n",
    "    decoded = Dense(input_shape[0], activation='sigmoid')(decoded)\n",
    "    \n",
    "    autoencoder = Model(inputs, decoded)\n",
    "    encoder = Model(inputs, encoded)\n",
    "    autoencoder.compile(optimizer=Adam(), loss='mse')\n",
    "    return autoencoder, encoder\n",
    "\n",
    "# Main Function\n",
    "def main():\n",
    "    eeg_file = 'mental-state.csv'\n",
    "    \n",
    "    print(\"\\nLoading EEG Dataset...\")\n",
    "    eeg_df = pd.read_csv(eeg_file)\n",
    "    X, y = preprocess_data(eeg_df)\n",
    "\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "    class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "    X_train, X_val = X[:int(0.8 * len(X))], X[int(0.8 * len(X)):]\n",
    "    y_train, y_val = y[:int(0.8 * len(y))], y[int(0.8 * len(y)):]\n",
    "\n",
    "    autoencoder, encoder = build_autoencoder((X_train.shape[1],))\n",
    "    autoencoder.fit(X_train, X_train, epochs=10, batch_size=32, validation_data=(X_val, X_val), verbose=1)\n",
    "\n",
    "    X_train_encoded = encoder.predict(X_train)\n",
    "    X_val_encoded = encoder.predict(X_val)\n",
    "\n",
    "    input_encoded = Input(shape=(X_train_encoded.shape[1],))\n",
    "    classifier_output = Dense(len(np.unique(y)), activation='softmax')(input_encoded)\n",
    "    classifier_model = Model(input_encoded, classifier_output)\n",
    "    classifier_model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    history = classifier_model.fit(X_train_encoded, y_train, epochs=10, batch_size=32, validation_data=(X_val_encoded, y_val), verbose=1)\n",
    "    \n",
    "    for epoch, acc in enumerate(history.history['accuracy'], 1):\n",
    "        print(f\"Epoch {epoch}: Training Accuracy = {acc:.4f}\")\n",
    "    \n",
    "    for epoch, val_acc in enumerate(history.history['val_accuracy'], 1):\n",
    "        print(f\"Epoch {epoch}: Validation Accuracy = {val_acc:.4f}\")\n",
    "    \n",
    "    y_pred = np.argmax(classifier_model.predict(X_val_encoded), axis=1)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    print(f\"\\nFinal Model Accuracy: {accuracy:.4f}\")\n",
    "    print(classification_report(y_val, y_pred))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c3beef0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eeg_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43meeg_file\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'eeg_file' is not defined"
     ]
    }
   ],
   "source": [
    "eeg_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b18adfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_csv(\"mental-state.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b4f688e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lag1_mean_0</th>\n",
       "      <th>lag1_mean_1</th>\n",
       "      <th>lag1_mean_2</th>\n",
       "      <th>lag1_mean_3</th>\n",
       "      <th>lag1_mean_d_h2h1_0</th>\n",
       "      <th>lag1_mean_d_h2h1_1</th>\n",
       "      <th>lag1_mean_d_h2h1_2</th>\n",
       "      <th>lag1_mean_d_h2h1_3</th>\n",
       "      <th>lag1_mean_q1_0</th>\n",
       "      <th>lag1_mean_q1_1</th>\n",
       "      <th>...</th>\n",
       "      <th>freq_669_3</th>\n",
       "      <th>freq_679_3</th>\n",
       "      <th>freq_689_3</th>\n",
       "      <th>freq_699_3</th>\n",
       "      <th>freq_709_3</th>\n",
       "      <th>freq_720_3</th>\n",
       "      <th>freq_730_3</th>\n",
       "      <th>freq_740_3</th>\n",
       "      <th>freq_750_3</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25.781648</td>\n",
       "      <td>33.836367</td>\n",
       "      <td>-92.769629</td>\n",
       "      <td>19.187957</td>\n",
       "      <td>-1.542262</td>\n",
       "      <td>0.197462</td>\n",
       "      <td>-119.561133</td>\n",
       "      <td>2.032654</td>\n",
       "      <td>21.596272</td>\n",
       "      <td>33.965587</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29.357891</td>\n",
       "      <td>26.792566</td>\n",
       "      <td>417.203910</td>\n",
       "      <td>19.472121</td>\n",
       "      <td>-38.797263</td>\n",
       "      <td>-16.897194</td>\n",
       "      <td>-29.368531</td>\n",
       "      <td>-9.055370</td>\n",
       "      <td>44.647424</td>\n",
       "      <td>40.893307</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001671</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>0.001122</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.001249</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28.451926</td>\n",
       "      <td>31.076434</td>\n",
       "      <td>72.231301</td>\n",
       "      <td>14.245938</td>\n",
       "      <td>-13.225057</td>\n",
       "      <td>-0.614138</td>\n",
       "      <td>-28.331698</td>\n",
       "      <td>-8.858742</td>\n",
       "      <td>31.450289</td>\n",
       "      <td>30.692883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>0.000909</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.001165</td>\n",
       "      <td>0.000616</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.282184</td>\n",
       "      <td>19.985184</td>\n",
       "      <td>16.220094</td>\n",
       "      <td>39.787312</td>\n",
       "      <td>1.847866</td>\n",
       "      <td>0.670216</td>\n",
       "      <td>-1.820355</td>\n",
       "      <td>20.220724</td>\n",
       "      <td>21.404679</td>\n",
       "      <td>20.777411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>0.005644</td>\n",
       "      <td>0.006891</td>\n",
       "      <td>0.010546</td>\n",
       "      <td>0.009583</td>\n",
       "      <td>0.011158</td>\n",
       "      <td>0.008853</td>\n",
       "      <td>0.004551</td>\n",
       "      <td>0.002287</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.431516</td>\n",
       "      <td>28.982168</td>\n",
       "      <td>27.540246</td>\n",
       "      <td>19.960398</td>\n",
       "      <td>2.491458</td>\n",
       "      <td>-6.020503</td>\n",
       "      <td>-1.071166</td>\n",
       "      <td>2.655259</td>\n",
       "      <td>16.295039</td>\n",
       "      <td>32.658163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001659</td>\n",
       "      <td>0.014379</td>\n",
       "      <td>0.014492</td>\n",
       "      <td>0.002949</td>\n",
       "      <td>0.004575</td>\n",
       "      <td>0.008305</td>\n",
       "      <td>0.007202</td>\n",
       "      <td>0.006957</td>\n",
       "      <td>0.009836</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2474</th>\n",
       "      <td>15.762328</td>\n",
       "      <td>19.113555</td>\n",
       "      <td>23.696867</td>\n",
       "      <td>7.568395</td>\n",
       "      <td>-6.503336</td>\n",
       "      <td>6.867187</td>\n",
       "      <td>-11.955396</td>\n",
       "      <td>-16.519912</td>\n",
       "      <td>19.838319</td>\n",
       "      <td>14.333094</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008537</td>\n",
       "      <td>0.008941</td>\n",
       "      <td>0.004102</td>\n",
       "      <td>0.003156</td>\n",
       "      <td>0.003659</td>\n",
       "      <td>0.010179</td>\n",
       "      <td>0.004591</td>\n",
       "      <td>0.013817</td>\n",
       "      <td>0.004536</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2475</th>\n",
       "      <td>34.675582</td>\n",
       "      <td>34.200645</td>\n",
       "      <td>-57.624820</td>\n",
       "      <td>-4.825609</td>\n",
       "      <td>7.382353</td>\n",
       "      <td>2.324416</td>\n",
       "      <td>-1.341208</td>\n",
       "      <td>-4.178625</td>\n",
       "      <td>26.383597</td>\n",
       "      <td>28.782987</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003324</td>\n",
       "      <td>0.003593</td>\n",
       "      <td>0.001702</td>\n",
       "      <td>0.003121</td>\n",
       "      <td>0.002686</td>\n",
       "      <td>0.001645</td>\n",
       "      <td>0.001770</td>\n",
       "      <td>0.001038</td>\n",
       "      <td>0.001973</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2476</th>\n",
       "      <td>29.813809</td>\n",
       "      <td>29.623031</td>\n",
       "      <td>-86.503988</td>\n",
       "      <td>7.532121</td>\n",
       "      <td>-19.581287</td>\n",
       "      <td>-0.628400</td>\n",
       "      <td>133.947160</td>\n",
       "      <td>-2.049096</td>\n",
       "      <td>45.484851</td>\n",
       "      <td>32.163999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000754</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.001079</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>59.453973</td>\n",
       "      <td>17.944332</td>\n",
       "      <td>-10.164238</td>\n",
       "      <td>42.568211</td>\n",
       "      <td>-1.300655</td>\n",
       "      <td>-19.993690</td>\n",
       "      <td>-54.331696</td>\n",
       "      <td>12.947622</td>\n",
       "      <td>55.203380</td>\n",
       "      <td>40.228490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003332</td>\n",
       "      <td>0.003557</td>\n",
       "      <td>0.004063</td>\n",
       "      <td>0.001662</td>\n",
       "      <td>0.002665</td>\n",
       "      <td>0.002353</td>\n",
       "      <td>0.003976</td>\n",
       "      <td>0.001660</td>\n",
       "      <td>0.003229</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2478</th>\n",
       "      <td>22.893855</td>\n",
       "      <td>30.412723</td>\n",
       "      <td>26.029590</td>\n",
       "      <td>14.249789</td>\n",
       "      <td>-7.101478</td>\n",
       "      <td>-0.551013</td>\n",
       "      <td>3.735563</td>\n",
       "      <td>-9.372750</td>\n",
       "      <td>30.411574</td>\n",
       "      <td>30.079904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002470</td>\n",
       "      <td>0.003917</td>\n",
       "      <td>0.002528</td>\n",
       "      <td>0.005357</td>\n",
       "      <td>0.004612</td>\n",
       "      <td>0.004503</td>\n",
       "      <td>0.003669</td>\n",
       "      <td>0.002316</td>\n",
       "      <td>0.004765</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2479 rows × 989 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      lag1_mean_0  lag1_mean_1  lag1_mean_2  lag1_mean_3  lag1_mean_d_h2h1_0  \\\n",
       "0       25.781648    33.836367   -92.769629    19.187957           -1.542262   \n",
       "1       29.357891    26.792566   417.203910    19.472121          -38.797263   \n",
       "2       28.451926    31.076434    72.231301    14.245938          -13.225057   \n",
       "3       21.282184    19.985184    16.220094    39.787312            1.847866   \n",
       "4       20.431516    28.982168    27.540246    19.960398            2.491458   \n",
       "...           ...          ...          ...          ...                 ...   \n",
       "2474    15.762328    19.113555    23.696867     7.568395           -6.503336   \n",
       "2475    34.675582    34.200645   -57.624820    -4.825609            7.382353   \n",
       "2476    29.813809    29.623031   -86.503988     7.532121          -19.581287   \n",
       "2477    59.453973    17.944332   -10.164238    42.568211           -1.300655   \n",
       "2478    22.893855    30.412723    26.029590    14.249789           -7.101478   \n",
       "\n",
       "      lag1_mean_d_h2h1_1  lag1_mean_d_h2h1_2  lag1_mean_d_h2h1_3  \\\n",
       "0               0.197462         -119.561133            2.032654   \n",
       "1             -16.897194          -29.368531           -9.055370   \n",
       "2              -0.614138          -28.331698           -8.858742   \n",
       "3               0.670216           -1.820355           20.220724   \n",
       "4              -6.020503           -1.071166            2.655259   \n",
       "...                  ...                 ...                 ...   \n",
       "2474            6.867187          -11.955396          -16.519912   \n",
       "2475            2.324416           -1.341208           -4.178625   \n",
       "2476           -0.628400          133.947160           -2.049096   \n",
       "2477          -19.993690          -54.331696           12.947622   \n",
       "2478           -0.551013            3.735563           -9.372750   \n",
       "\n",
       "      lag1_mean_q1_0  lag1_mean_q1_1  ...  freq_669_3  freq_679_3  freq_689_3  \\\n",
       "0          21.596272       33.965587  ...    0.000230    0.000351    0.000547   \n",
       "1          44.647424       40.893307  ...    0.001671    0.000740    0.001122   \n",
       "2          31.450289       30.692883  ...    0.000748    0.000569    0.000327   \n",
       "3          21.404679       20.777411  ...    0.000990    0.005644    0.006891   \n",
       "4          16.295039       32.658163  ...    0.001659    0.014379    0.014492   \n",
       "...              ...             ...  ...         ...         ...         ...   \n",
       "2474       19.838319       14.333094  ...    0.008537    0.008941    0.004102   \n",
       "2475       26.383597       28.782987  ...    0.003324    0.003593    0.001702   \n",
       "2476       45.484851       32.163999  ...    0.000754    0.000508    0.000263   \n",
       "2477       55.203380       40.228490  ...    0.003332    0.003557    0.004063   \n",
       "2478       30.411574       30.079904  ...    0.002470    0.003917    0.002528   \n",
       "\n",
       "      freq_699_3  freq_709_3  freq_720_3  freq_730_3  freq_740_3  freq_750_3  \\\n",
       "0       0.000381    0.000350    0.000453    0.000442    0.000325    0.000209   \n",
       "1       0.000521    0.000624    0.000439    0.001249    0.000727    0.000801   \n",
       "2       0.000197    0.000833    0.000909    0.000699    0.001165    0.000616   \n",
       "3       0.010546    0.009583    0.011158    0.008853    0.004551    0.002287   \n",
       "4       0.002949    0.004575    0.008305    0.007202    0.006957    0.009836   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2474    0.003156    0.003659    0.010179    0.004591    0.013817    0.004536   \n",
       "2475    0.003121    0.002686    0.001645    0.001770    0.001038    0.001973   \n",
       "2476    0.000701    0.000797    0.001096    0.000388    0.000529    0.001079   \n",
       "2477    0.001662    0.002665    0.002353    0.003976    0.001660    0.003229   \n",
       "2478    0.005357    0.004612    0.004503    0.003669    0.002316    0.004765   \n",
       "\n",
       "      Label  \n",
       "0       2.0  \n",
       "1       2.0  \n",
       "2       2.0  \n",
       "3       1.0  \n",
       "4       2.0  \n",
       "...     ...  \n",
       "2474    0.0  \n",
       "2475    2.0  \n",
       "2476    2.0  \n",
       "2477    2.0  \n",
       "2478    1.0  \n",
       "\n",
       "[2479 rows x 989 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c7aa7b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftConfig, PeftModel\n\u001b[0;32m      5\u001b[0m base_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mistral-7B-Instruct-v0.2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftConfig, PeftModel\n",
    "\n",
    "base_model = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "adapter = \"GRMenon/mental-health-mistral-7b-instructv0.2-finetuned-V2\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model,\n",
    "    add_bos_token=True,\n",
    "    trust_remote_code=True,\n",
    "    padding_side='left'\n",
    ")\n",
    "\n",
    "# Create peft model using base_model and finetuned adapter\n",
    "config = PeftConfig.from_pretrained(adapter)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,\n",
    "                                             load_in_4bit=True,\n",
    "                                             device_map='auto',\n",
    "                                             torch_dtype='auto')\n",
    "model = PeftModel.from_pretrained(model, adapter)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Prompt content:\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hey Connor! I have been feeling a bit down lately.I could really use some advice on how to feel better?\"}\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(conversation=messages,\n",
    "                                          tokenize=True,\n",
    "                                          add_generation_prompt=True,\n",
    "                                          return_tensors='pt').to(device)\n",
    "output_ids = model.generate(input_ids=input_ids, max_new_tokens=512, do_sample=True, pad_token_id=2)\n",
    "response = tokenizer.batch_decode(output_ids.detach().cpu().numpy(), skip_special_tokens = True)\n",
    "\n",
    "# Model response: \n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76a2f10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\hi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in c:\\users\\hi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.28.2)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\hi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Downloading transformers-4.48.2-py3-none-any.whl (9.7 MB)\n",
      "   ---------------------------------------- 0.0/9.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/9.7 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.8/9.7 MB 2.4 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.3/9.7 MB 2.5 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.8/9.7 MB 2.3 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 2.1/9.7 MB 2.3 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 2.9/9.7 MB 2.3 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.4/9.7 MB 2.5 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 4.2/9.7 MB 2.6 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 5.0/9.7 MB 2.7 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 5.5/9.7 MB 2.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 6.0/9.7 MB 2.7 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 6.8/9.7 MB 2.8 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 7.6/9.7 MB 2.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.1/9.7 MB 2.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.7/9.7 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.2/9.7 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.7/9.7 MB 2.8 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.28.1-py3-none-any.whl (464 kB)\n",
      "Downloading safetensors-0.5.2-cp38-abi3-win_amd64.whl (303 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.5/2.4 MB 2.4 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.4 MB 2.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.6/2.4 MB 2.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.1/2.4 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 2.6 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.28.1 safetensors-0.5.2 tokenizers-0.21.0 transformers-4.48.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3 -> 25.0\n",
      "[notice] To update, run: C:\\Users\\HI\\AppData\\Local\\Programs\\Python\\Python311\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6b2406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
